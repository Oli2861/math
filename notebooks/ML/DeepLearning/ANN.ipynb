{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Artificial Neural Network ANN\n",
    "Objective: MNIST handwritten digits classifications using a Artificial Neural Network (ANN) without the usage of any libraries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Callable\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from datetime import time\n",
    "from typing import Counter\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from oli.ml.Activation_functions import relu\n",
    "from oli.math.math_utility import pretty_print_matrix\n",
    "from oli.ml.Activation_functions import relu_derivative\n",
    "from oli.ml.Activation_functions import softmax\n",
    "from oli.ml.Loss import mean_squared_error_loss_categorical\n",
    "from oli.ml.Loss import categorical_cross_entropy_loss\n",
    "from oli.ml.Loss import derivative_categorical_cross_entropy_loss\n",
    "from oli.ml.utility_functions import argmax"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:20:34.569511900Z",
     "start_time": "2023-10-08T11:20:29.961810100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Image:\n",
    "    pixels: list[list[int]]\n",
    "    height: int\n",
    "    width: int\n",
    "\n",
    "    def __init__(self, pixels: list[list[int]]):\n",
    "        self.pixels = pixels\n",
    "        self.height = len(pixels)\n",
    "        self.width = len(pixels[0])\n",
    "\n",
    "    def print(self):\n",
    "        pretty_print_matrix(self.pixels, label=f\"Image with dimensions width = {self.width} x height = {self.height}\",\n",
    "                            max_length=3)\n",
    "\n",
    "    def get_linearized(self) -> list[int]:\n",
    "        res: list[int] = []\n",
    "        for row in self.pixels:\n",
    "            for item in row:\n",
    "                res.append(item)\n",
    "        return res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:20:34.670513300Z",
     "start_time": "2023-10-08T11:20:34.560513600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class MNISTDataset:\n",
    "    images: list[Image]\n",
    "    labels: list[int]\n",
    "\n",
    "    def __init__(self, images: list[Image], labels: list[int]):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "        if len(images) != len(labels):\n",
    "            raise Exception(\"Amount of images doesnt match amount of labels.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return ((self.images[i], self.labels[i]) for i in range(len(self.images)))\n",
    "\n",
    "    def get_linearized_images(self) -> list[list[int]]:\n",
    "        return [img.get_linearized() for img in self.images]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:20:34.766515300Z",
     "start_time": "2023-10-08T11:20:34.671514600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:20:34.863514500Z",
     "start_time": "2023-10-08T11:20:34.766515300Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_image_file(path: str) -> list[Image]:\n",
    "    file_stream = open(path, \"rb\")\n",
    "    # Offset 0 - 4 --> 4 bytes\n",
    "    magic_number: bytes = file_stream.read(4)\n",
    "    magic_number: int = int.from_bytes(magic_number, byteorder=\"big\", signed=False)\n",
    "\n",
    "    # Offset 4 - 8 --> 4 bytes\n",
    "    number_of_images: bytes = file_stream.read(4)\n",
    "    number_of_images: int = int.from_bytes(number_of_images, byteorder=\"big\", signed=False)\n",
    "\n",
    "    # Offset 8 - 12 --> 4 bytes\n",
    "    number_of_rows: bytes = file_stream.read(4)\n",
    "    number_of_rows: int = int.from_bytes(number_of_rows, byteorder=\"big\", signed=False)\n",
    "\n",
    "    # Offset 8 - 12 --> 4 bytes\n",
    "    number_of_columns: bytes = file_stream.read(4)\n",
    "    number_of_columns: int = int.from_bytes(number_of_columns, byteorder=\"big\", signed=False)\n",
    "\n",
    "    print(\n",
    "        f\"Loading images:\\tMagic number: {magic_number}, Number of images: {number_of_images}, Number of rows: {number_of_rows}, Number of columns: {number_of_columns}\")\n",
    "\n",
    "    images: list[Image] = []\n",
    "    count = 0\n",
    "    for image_number in range(number_of_images):\n",
    "        pixels: list[list[int]] = [[0 for n in range(number_of_columns)] for i in range(number_of_rows)]\n",
    "        for row_number in range(number_of_rows):\n",
    "            for column_number in range(number_of_columns):\n",
    "                pixel: bytes = file_stream.read(1)\n",
    "                pixel: int = int.from_bytes(pixel, byteorder=\"big\", signed=False)\n",
    "                pixels[row_number][column_number] = pixel\n",
    "        images.append(Image(pixels))\n",
    "        if image_number % 10000 == 0:\n",
    "            print(\"Loaded image number\", image_number)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def read_label_file(path: str):\n",
    "    file_stream = open(path, \"rb\")\n",
    "    # Offset 0 - 4 --> 4 bytes\n",
    "    magic_number: bytes = file_stream.read(4)\n",
    "    magic_number: int = int.from_bytes(magic_number, byteorder=\"big\", signed=False)\n",
    "\n",
    "    # Offset 4 - 8 --> 4 bytes\n",
    "    number_of_items: bytes = file_stream.read(4)\n",
    "    number_of_items: int = int.from_bytes(number_of_items, byteorder=\"big\", signed=False)\n",
    "\n",
    "    print(f\"Loading labels:\\tMagic number: {magic_number}, Number of items: {number_of_items}\")\n",
    "\n",
    "    items: list[int] = []\n",
    "    for item_index in range(number_of_items):\n",
    "        item: bytes = file_stream.read(1)\n",
    "        item: int = int.from_bytes(item, byteorder=\"big\", signed=False)\n",
    "        items.append(item)\n",
    "\n",
    "    return items"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:20:34.972514Z",
     "start_time": "2023-10-08T11:20:34.867544400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images:\tMagic number: 2051, Number of images: 60000, Number of rows: 28, Number of columns: 28\n",
      "Loaded image number 0\n",
      "Loaded image number 10000\n",
      "Loaded image number 20000\n",
      "Loaded image number 30000\n",
      "Loaded image number 40000\n",
      "Loaded image number 50000\n",
      "Loading labels:\tMagic number: 2049, Number of items: 60000\n"
     ]
    }
   ],
   "source": [
    "train_images = read_image_file(\"../../../data/mnist/train-images.idx3-ubyte\")\n",
    "train_labels = read_label_file(\"../../../data/mnist/train-labels.idx1-ubyte\")\n",
    "train_set: MNISTDataset = MNISTDataset(train_images, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:21:02.488922600Z",
     "start_time": "2023-10-08T11:20:34.973538900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images:\tMagic number: 2051, Number of images: 10000, Number of rows: 28, Number of columns: 28\n",
      "Loaded image number 0\n",
      "Loading labels:\tMagic number: 2049, Number of items: 10000\n"
     ]
    }
   ],
   "source": [
    "test_images = read_image_file(\"../../../data/mnist/t10k-images.idx3-ubyte\")\n",
    "test_labels = read_label_file(\"../../../data/mnist/t10k-labels.idx1-ubyte\")\n",
    "test_set: MNISTDataset = MNISTDataset(test_images, test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:21:06.750923300Z",
     "start_time": "2023-10-08T11:21:02.488922600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:21:06.763921Z",
     "start_time": "2023-10-08T11:21:06.752921Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility: Matrix multiplication"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def multiplication(A: list[list[float]], B: list[list[float]]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Function to multiply two 2d matrices.\n",
    "    :param A: First matrix.\n",
    "    :param B: Second matrix.\n",
    "    :return: Matrix product.\n",
    "    \"\"\"\n",
    "    if len(A[0]) != len(B):\n",
    "        raise Exception(\n",
    "            f\"Multiplication is only possible if the number of columns of A corresponds to the number of rows in B. Columns of A: {len(A[0])} Rows of B: {len(B)}\")\n",
    "    m = len(A)  # Rows of A\n",
    "    n = len(A[0])  # Columns of A\n",
    "    n = len(B)  # Rows of B\n",
    "    p = len(B[0])  # Columns of B\n",
    "    C = [[0 for _ in range(p)] for _ in range(m)]\n",
    "\n",
    "    for y in range(0, m):\n",
    "        for x in range(0, p):\n",
    "            C[y][x] = 0\n",
    "            for u in range(0, n):\n",
    "                a_yu = A[y][u]\n",
    "                b_ux = B[u][x]\n",
    "                # print(f\"i: {i}, j: {j}, u: {u}, a_iu: {a_iu}, b_uj: {b_uj}\")\n",
    "                C[y][x] += a_yu * b_ux\n",
    "\n",
    "    return C"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:21:06.934922100Z",
     "start_time": "2023-10-08T11:21:06.769922200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  3.8000000000000003 4.4 5.0 5.6 \n",
      "  8.3 9.8 11.3 12.799999999999999 \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "A = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "]\n",
    "B = [\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2],\n",
    "]\n",
    "pretty_print_matrix(multiplication(A, B))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:21:07.075922800Z",
     "start_time": "2023-10-08T11:21:06.938921500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear layer\n",
    "- Activation function $g(x)$\n",
    "- Amount of inputs ($x$): $n$ (including bias)\n",
    "- Amount of neurons ($l$): $m$\n",
    "- For each node: Weights for each incoming edge $w_{x_{0..n}}$ --> For all nodes combined $W$ must have dimensionality $n \\times m$\n",
    "\n",
    "## Feed forward\n",
    "Prediction node $l_0$: $x_0 \\cdot w_{l_0}_{x_0} + x_1 \\cdot w_{l_0}_{x_1} + ... + x_n \\cdot w_{l_0}_{x_n}$\n",
    "Prediction node $l_1$: $x_0 \\cdot w_{l_1}_{x_0} + x_1 \\cdot w_{l_1}_{x_1} + ... + x_n \\cdot w_{l_1}_{x_n}$\n",
    "Prediction node $l_2$: $x_0 \\cdot w_{l_1}_{x_0} + x_1 \\cdot w_{l_2}_{x_1} + ... + x_n \\cdot w_{l_2}_{x_n}$\n",
    "--> Feed forward as a matrix multiplication:\n",
    "$$\n",
    "\\hat y = g(\n",
    "\\begin{pmatrix}\n",
    "    x_0\\\\\n",
    "    x_1\\\\\n",
    "    ...\\\\\n",
    "    x_n\n",
    "\\end{pmatrix}\n",
    "\\begin{bmatrix}\n",
    "\tw_{l_0, x_0} & w_{l_1, x_0} & ... & w_{l_m, x_0} \\\\\n",
    "\tw_{l_0, x_1} & w_{l_1, x_1} & ... & w_{l_m, x_1} \\\\\n",
    "\t\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\tw_{l_0, x_n} & w_{l_1, x_n} & ...& w_{l_m, x_n}\n",
    "\\end{bmatrix})\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "[3b1b explanation](https://www.3blue1brown.com/lessons/backpropagation-calculus)\n",
    "\n",
    "### Symbols & coding\n",
    "| Symbol                | Meaning                                                            |\n",
    "|-----------------------|--------------------------------------------------------------------|\n",
    "| $w$                   | Weight                                                             |\n",
    "| $b$                   | Bias                                                               |\n",
    "| $z$                   | Matrix multiplication product                                      |\n",
    "| $a$                   | Activation                                                         |\n",
    "| $C$                   | Total cost of network (Average of costs for each training example) |\n",
    "| $C_0$ | Cost of sample                                                     |\n",
    "\n",
    "|Indices| Meaning                     |\n",
    "|-------|-----------------------------|\n",
    "| $j$| Current neuron of layer L   |\n",
    "| $k$| Current neuron of layer L-1 |\n",
    "\n",
    "### Objective\n",
    "Goal: How sensitive is cost $C_0$ to changes in $w^{(L)}$: $\\frac{\\partial C_0}{\\partial w^{(L)}}$\n",
    "Sensitivity given by the chain rule\n",
    "\n",
    "### Chain rule\n",
    "$$\\frac{\\partial C_0}{\\partial w^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\frac{\\partial C_0}{\\partial a^{(L)}} = a^{(L-1)}g'(z^{(L)})2(a^{(L)}-y)$$\n",
    "\n",
    "##### Chain rule components\n",
    "| Description                                                  | Formula                                                   | Per neuron                                                            |\n",
    "|--------------------------------------------------------------|-----------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| How much do changes in $w^{(L)}$ affect changes in $z^{(L)}$ | $\\frac{\\partial z^{(L)}}{\\partial w^{(L)}} = a^{(L-1)}$   | $\\frac{\\partial z_j^{(L)}}{\\partial w_{ji}^{(L)}} = a_i^{(L-1)}$      |\n",
    "| How much do changes in $z^{(L)}$ affect changes in $a^{(L)}$ | $\\frac{\\partial a^{(L)}}{\\partial z^{(L)}} = g'(z^{(L)})$ | $\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}} = g'(z_{j}^{(L)})$ |\n",
    "| How much do changes in $a^{(L)}$ affect changes in $C_0$     | $\\frac{\\partial C_0}{\\partial a^{(L)}} = 2(a^{(L)} - y)$  | $\\frac{\\partial C_0}{\\partial a_j^{(L)}} = 2(a_j^{(L)} - y)$          |\n",
    "| Activation                                                   | $a^{(L)} = g(z^{(L)}) = g(w^{(L)} \\cdot a^{(L-1)})$       |                                                                       |\n",
    "\n",
    "### Upstream error calculation:\n",
    "\n",
    "| Description                                                                                                                                                  | Formula                                                                                                                                                                                                   |\n",
    "|--------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Previous neuron influences multiple neurons in the following layer --> Sum the error up by summing up the chain rule expressions (one per path of influence) | $\\frac{\\partial C_0}{\\partial a_k^{(L-1)}} = \\sum_{j=0}^{n_{L}} \\frac{\\partial z_j^{(L)}}{\\partial a_k^{(L-1)}} \\frac{\\partial a_j^{(L)}}{\\partial z_j^{(L)}} \\frac{\\partial C_0}{\\partial a_j^{(L)}}$  |\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: list[list[float]]) -> list[list[float]]:\n",
    "        pass\n",
    "\n",
    "    # @abstractmethod\n",
    "    # def backprop(self, previous_activation: list[float], label: float, learning_rate: float):\n",
    "    #     pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:21:07.232920700Z",
     "start_time": "2023-10-08T11:21:07.078920900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    activation_function: Callable[[float], float]\n",
    "    derivative_activation_function: Callable[[float], float]\n",
    "    derivative_cost_function: Callable[[float, float], float]\n",
    "    bias: float = 1\n",
    "    W = list[list[float]]\n",
    "    neurons: int\n",
    "    inputs: int\n",
    "    test_mode: bool\n",
    "\n",
    "    # For backpropagation\n",
    "    all_activations_a: list[list[float]]\n",
    "    all_multiplication_results_z: list[list[float]]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            neurons: int,\n",
    "            inputs: int,\n",
    "            activation_function: Callable[[float], float],\n",
    "            derivative_activation_function: Callable[[float], float],\n",
    "            derivative_cost_function: Callable[[float, float], float],\n",
    "            test_mode: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a linear layer with a fixed number of neurons for a fixed number of inputs.\n",
    "        :param neurons: The number of neurons the linear layer shall contain.\n",
    "        :param inputs: The number of inputs.\n",
    "        :param activation_function: The activation function of the linear layer.\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.inputs = inputs\n",
    "        self.activation_function = activation_function\n",
    "        self.derivative_activation_function = derivative_activation_function\n",
    "        self.derivative_cost_function = derivative_cost_function\n",
    "        self.test_mode = test_mode\n",
    "        self.all_activations_a = []\n",
    "        self.all_multiplication_results_z = []\n",
    "\n",
    "        if test_mode:\n",
    "            self.W = [[0.5 for m in range(neurons)] for n in range((inputs + 1))]\n",
    "        else:\n",
    "            self.W = [[random.random() * 2 - 1 for m in range(neurons)] for n in range((inputs + 1))]\n",
    "\n",
    "        assert len(self.W) == inputs + 1\n",
    "        assert len(self.W[0]) == neurons\n",
    "\n",
    "    def forward(self, x: list[float]) -> list[list[float]]:\n",
    "        \"\"\"\n",
    "        Forward pass through the linear layer by multiplying the weights (including bias) with the data (padded by a additional unit for the bias).\n",
    "        :param x: Data used to make a prediction.\n",
    "        :return: Prediction of the linear layer.\n",
    "        \"\"\"\n",
    "        # Add bias to input to allow the forward pass to be treated as a matrix multiplication.\n",
    "        assert len(x) == self.inputs\n",
    "        x.insert(0, self.bias)\n",
    "\n",
    "        if self.test_mode: print(\n",
    "            f\"Input count: {len(x):^6}\\tWeight dimensions count: {len(self.W):^6} x {len(self.W[0]):^6} = {len(self.W) * len(self.W[0])}\")\n",
    "\n",
    "        x: list[list[float]] = [x]\n",
    "        # Multiply weights with input (bias inserted into inputs)\n",
    "        multi: list[float] = multiplication(x, self.W)[0]\n",
    "        assert len(multi) == self.neurons\n",
    "        # Save result of the multiplication (z) for backpropagation\n",
    "        self.all_multiplication_results_z.append(multi)\n",
    "\n",
    "        # Apply activation function\n",
    "        activation = [self.activation_function(curr) for curr in multi]\n",
    "        # Save activation (a) for backpropagation\n",
    "        self.all_activations_a.append(activation)\n",
    "\n",
    "        return activation\n",
    "\n",
    "    def backprop(\n",
    "            self,\n",
    "            previous_activation: list[list[float]],\n",
    "            learning_rate: float,\n",
    "            labels: list[float] | None,\n",
    "            predictions: list[list[float]] |None,\n",
    "            layer_index: int,\n",
    "            activation_cost_effect: list[list[float]] | None = None,\n",
    "            print_info: bool = True,\n",
    "    ) -> list[list[float]]:\n",
    "        \"\"\"\n",
    "        Backpropagate the error through the neural network. Adjust the weights based on the learning rate and the error received at the corresponding linear layer.\n",
    "        :param layer_index:\n",
    "        :param labels:\n",
    "        :param previous_activation: Activation received from the previous layer / input: a^{(L-1)}\n",
    "        :param learning_rate: Describes how large the gradient steps are.\n",
    "        :param predictions:\n",
    "        :param activation_cost_effect: Costs induced by the activation of this layer. If this layer is the last layer set to None in order to calculate the effect based on the loss.\n",
    "        :return: Activation cost effect of the upstream layer.\n",
    "        \"\"\"\n",
    "        activation_cost_effect_for_upstream_layer_for_all_examples = []\n",
    "        average_w_on_Ck_effect_for_all_examples = []\n",
    "        for neuron_index_j in range(self.neurons):\n",
    "            for previous_activation_index_k in range(len(previous_activation[0])):\n",
    "                w_on_Ck_effect_sum = 0\n",
    "                batch_size = len(previous_activation)\n",
    "                for example_index in range(batch_size):\n",
    "                    # w_on_Ck_effect: Effect of weight on cost for a single example.\n",
    "                    # activation_cost_effect_for_upstream_layer: Effect of activation on cost for a single example.\n",
    "                    (w_on_Ck_effect, activation_cost_effect_for_upstream_layer) =  self.apply_chain_rule(\n",
    "                        previous_activation=previous_activation[example_index],\n",
    "                        multiplication_result_z=self.all_multiplication_results_z[example_index],\n",
    "                        label=labels[example_index] if labels is not None else None,\n",
    "                        prediction=predictions[example_index] if predictions is not None else None,\n",
    "                        activation_cost_effect=activation_cost_effect[example_index] if activation_cost_effect is not None else None,\n",
    "                        neuron_index_j=neuron_index_j,\n",
    "                        previous_activation_index_k=previous_activation_index_k,\n",
    "                        layer_index=layer_index\n",
    "                    )\n",
    "                    w_on_Ck_effect_sum += w_on_Ck_effect\n",
    "                    activation_cost_effect_for_upstream_layer_for_all_examples.append(activation_cost_effect_for_upstream_layer)\n",
    "\n",
    "                # Adjust weights by the average cost sensitivity of the weight on the cost.\n",
    "                self.W[previous_activation_index_k][neuron_index_j] = self.W[previous_activation_index_k][neuron_index_j] - learning_rate * w_on_Ck_effect_sum\n",
    "                average_w_on_Ck_effect_for_all_examples.append(w_on_Ck_effect_sum)\n",
    "                # Progress indicator\n",
    "                if print_info and neuron_index_j == 0 and previous_activation_index_k == 0:\n",
    "                    print(f\"Weight change: {learning_rate * (w_on_Ck_effect_sum)} (lr={learning_rate}, w_onCk_effect_sum={w_on_Ck_effect_sum}, batch_size={batch_size}) for weight W[{previous_activation_index_k}][{neuron_index_j}]\")\n",
    "                    print(f\"Sum of effect of weight change on costs: {w_on_Ck_effect_sum} (for Neuron index j={neuron_index_j} Previous activation index k={previous_activation_index_k} Example index={example_index})\")\n",
    "\n",
    "            # if print_info and neuron_index_j % (self.neurons / 4) == 0 or neuron_index_j == self.neurons - 1:\n",
    "                # print(f\"Done with {neuron_index_j / self.neurons * 100}% of the neurons in this layer.\")\n",
    "        if print_info:\n",
    "            print(f\"Average (of layer) of the sum of effect of weight change cost: {sum(average_w_on_Ck_effect_for_all_examples)/len(average_w_on_Ck_effect_for_all_examples)}\\n\")\n",
    "        self.clear_cached_results()\n",
    "        return activation_cost_effect_for_upstream_layer_for_all_examples\n",
    "\n",
    "    def apply_chain_rule(\n",
    "            self,\n",
    "            previous_activation: list[float],\n",
    "            multiplication_result_z: list[float],\n",
    "            label: float,\n",
    "            prediction: list[float],\n",
    "            activation_cost_effect: list[float],\n",
    "            neuron_index_j: int,\n",
    "            previous_activation_index_k: int,\n",
    "            layer_index: int\n",
    "    ):\n",
    "        activation_cost_effect_for_upstream_layer = [0 for index in range(len(previous_activation))]\n",
    "        # Calculate chain rule components\n",
    "        # Effect of a weight change on the matrix multiplication product\n",
    "        w_on_z_effect: float = self.effect_of_weights_on_matrix_multiplication_product(previous_activation[neuron_index_j])\n",
    "        # Effect of matrix multiplication product change on activation\n",
    "        z_on_a_effect: float = self.effect_of_matrix_multiplication_product_on_activation(multiplication_result_z[neuron_index_j])\n",
    "        # Effect of activation change on costs\n",
    "        a_on_c0_effect = self.effect_of_activation_on_cost(label, prediction, activation_cost_effect, neuron_index_j)\n",
    "\n",
    "        # Chain rule: Effect of changes of the weights on the costs\n",
    "        cost_sensitivity_with_respect_to_weight_changes: float = w_on_z_effect * z_on_a_effect * a_on_c0_effect\n",
    "\n",
    "        if layer_index > 0:\n",
    "            # Effect of previous activation on matrix product\n",
    "            prev_a_on_z_effect: float = self.effect_of_previous_activation_on_matrix_product(neuron_index_j, previous_activation_index_k)\n",
    "            activation_cost_effect_for_upstream_layer[previous_activation_index_k] += prev_a_on_z_effect * z_on_a_effect * a_on_c0_effect\n",
    "            return cost_sensitivity_with_respect_to_weight_changes, activation_cost_effect_for_upstream_layer\n",
    "        else:\n",
    "            return cost_sensitivity_with_respect_to_weight_changes, None\n",
    "\n",
    "    def effect_of_weights_on_matrix_multiplication_product(self, previous_activation: float) -> float:\n",
    "        \"\"\"\n",
    "        The effect of the weights is given by the previous activation a^{(L-1)}.\n",
    "        \"\"\"\n",
    "        return previous_activation\n",
    "\n",
    "    def effect_of_matrix_multiplication_product_on_activation(self, matrix_multiplication_result_z_j: float) -> float:\n",
    "        \"\"\"\n",
    "        The effect of the matrix multiplication product on the activation is the derivative of the activation function applied to the matrix multiplication product: \\sigma ' (z^{(L)}).\n",
    "        \"\"\"\n",
    "        return self.derivative_activation_function(matrix_multiplication_result_z_j)\n",
    "\n",
    "    def effect_of_activation_on_cost(self, label: float, predictions: list[float], activation_cost_effect: list[float], neuron_index_j: int):\n",
    "        \"\"\"\n",
    "        The effect of the activation on the cost is the derivative of the loss function. e.g. 2(a^{(L)} -y) for squared error loss\n",
    "        \"\"\"\n",
    "        if activation_cost_effect is None and label is not None and predictions is not None:\n",
    "            return self.derivative_cost_function(predictions[neuron_index_j], label)\n",
    "        elif activation_cost_effect is not None:\n",
    "            return activation_cost_effect[neuron_index_j]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"Illegal state:\\nactivation_cost_effect: {activation_cost_effect}\\nlabel: {label}\\npredictions:{predictions}\")\n",
    "\n",
    "    def effect_of_previous_activation_on_matrix_product(self, neuron_index_j: int, previous_activation_index_k: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the effect of the previous activation on the matrix multiplication product.\n",
    "        It is the activation at the index of the relevant weight: Row index in the weight matrix denotes the feature index, column index denotes the neurons of the layer.\n",
    "        \"\"\"\n",
    "        return self.W[previous_activation_index_k][neuron_index_j]\n",
    "\n",
    "    def clear_cached_results(self):\n",
    "        self.all_activations_a = []\n",
    "        self.all_multiplication_results_z = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:21:07.406921200Z",
     "start_time": "2023-10-08T11:21:07.235922200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    layers: list[LinearLayer]\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.layers = args\n",
    "\n",
    "    def train(self, X: list[list[float]], y: list[int], epochs: int, batch_size: int, learning_rate: float):\n",
    "        \"\"\"\n",
    "        :param X: Data to train on.\n",
    "        :param y: Labels corresponding to the data.\n",
    "        :param epochs: The number of epochs to train the NN for.\n",
    "        :param batch_size: Size of a training batch.\n",
    "        :param learning_rate: The learning rate (amount by which the weights are adjusted).\n",
    "        \"\"\"\n",
    "        # Prepare for confusion matrix\n",
    "\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            for index in range(len(X)):\n",
    "                for batch_index in range(int(len(X) / batch_size)):\n",
    "                    batch_start = batch_index * batch_size\n",
    "                    batch_end = batch_start + batch_size\n",
    "                    x_batch: list[list[float]] = X[batch_start:batch_end]\n",
    "                    y_batch: list[int] = y[batch_start:batch_end]\n",
    "                    batch_predictions: list[int] = []\n",
    "                    batch_probabilities: list[list[float]] = []\n",
    "                    batch_loss = []\n",
    "                    for index in range(len(x_batch)):\n",
    "                        activation: list[float] = self.predict(x_batch[index])\n",
    "                        probability: list[float] = softmax(activation)\n",
    "                        batch_probabilities.append(probability)\n",
    "                        curr_loss = categorical_cross_entropy_loss(probability, y_batch[index])\n",
    "                        batch_loss.append(curr_loss)\n",
    "                        # print(f\"Epoch: {epoch} Batch: {batch_index} Index: {index} Loss: {curr_loss} Label: {y_batch[index]}\\nActivation: {activation}\\nProbability: {probability}\\n\")\n",
    "                        batch_predictions.append(argmax(probability))\n",
    "                    self.backprop(learning_rate, x_batch, y_batch, batch_probabilities)\n",
    "                    batch_loss = sum(batch_loss)/len(batch_loss)\n",
    "                    loss_history.append(batch_loss)\n",
    "                    print(f\"Batch {batch_index} / {int(len(X) / batch_size)} completed of epoch {epoch} / {epochs}. Loss: {batch_loss}, Loss history: {loss_history}\")\n",
    "                    print(f\"Batch label balance: {Counter[int](y_batch)}\")\n",
    "                    if batch_loss < 0.3:\n",
    "                        print(\"Loss is below 0.3, stopping training.\")\n",
    "                        return\n",
    "        print(loss_history)\n",
    "\n",
    "    def backprop(self, learning_rate: float, x: list[list[float]], y: list[int], predictions: list[list[float]]):\n",
    "        activation_cost_effect = None\n",
    "        for index in reversed(range(len(self.layers))):\n",
    "            isLastLayer = index == len(self.layers) - 1\n",
    "            isFirstLayer = index == 0\n",
    "            print(f\"Backpropagating layer {index}\")\n",
    "            activation_cost_effect = self.layers[index].backprop(\n",
    "                previous_activation=x if isFirstLayer else self.layers[index - 1].all_activations_a,\n",
    "                predictions=predictions if isLastLayer else None,\n",
    "                labels=y if isLastLayer else None,\n",
    "                learning_rate=learning_rate,\n",
    "                activation_cost_effect=activation_cost_effect,\n",
    "                layer_index=index\n",
    "            )\n",
    "\n",
    "    def predict(self, x: list[float]) -> list[float]:\n",
    "        curr_x: list[float] = x\n",
    "        for (index, layer) in enumerate(self.layers):\n",
    "            curr_x = layer.forward(curr_x)\n",
    "        return curr_x\n",
    "\n",
    "    def predict_multiple(self, X: list[list[float]]) -> list[float]:\n",
    "        predictions: list[float] = []\n",
    "        for x in X:\n",
    "            pred = self.predict(x)\n",
    "            probs = softmax(pred)\n",
    "            predictions.append(probs)\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T11:21:07.564921300Z",
     "start_time": "2023-10-08T11:21:07.407920800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backpropagating layer 3\n",
      "Weight change: -0.1541437094767194 (lr=0.001, w_onCk_effect_sum=-154.1437094767194, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -154.1437094767194 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1367569.815510483\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: 0.01932378528340894 (lr=0.001, w_onCk_effect_sum=19.32378528340894, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: 19.32378528340894 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: 0.8051577201420392\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -8.843044488715365e-06 (lr=0.001, w_onCk_effect_sum=-0.008843044488715365, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -0.008843044488715365 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -0.0003684601870298057\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -4.416584130682295e-06 (lr=0.001, w_onCk_effect_sum=-0.004416584130682295, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -0.004416584130682295 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -3.450456352095583e-05\n",
      "\n",
      "Batch 0 / 234 completed of epoch 0 / 1. Loss: 17.55464100678961, Loss history: [17.55464100678961]\n",
      "Batch label balance: Counter({1: 35, 0: 30, 3: 30, 9: 26, 7: 26, 2: 25, 4: 24, 6: 24, 8: 19, 5: 17})\n",
      "Backpropagating layer 3\n",
      "Weight change: -0.9769987254124136 (lr=0.001, w_onCk_effect_sum=-976.9987254124136, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -976.9987254124136 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -2964071.8752462603\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: 0.007034729703892839 (lr=0.001, w_onCk_effect_sum=7.034729703892839, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: 7.034729703892839 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: 0.29311373766220167\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -9.857788652589012e-05 (lr=0.001, w_onCk_effect_sum=-0.09857788652589011, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -0.09857788652589011 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -0.004107411938578744\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -2.0345098690997e-05 (lr=0.001, w_onCk_effect_sum=-0.020345098690997, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -0.020345098690997 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -0.00015894608352341657\n",
      "\n",
      "Batch 1 / 234 completed of epoch 0 / 1. Loss: 9.958949221937699, Loss history: [17.55464100678961, 9.958949221937699]\n",
      "Batch label balance: Counter({1: 34, 9: 30, 4: 28, 7: 28, 2: 27, 5: 24, 6: 22, 3: 22, 8: 21, 0: 20})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.1559938192270574 (lr=0.001, w_onCk_effect_sum=-1155.9938192270574, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1155.9938192270574 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -2965075.4827549453\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -0.1143463400974954 (lr=0.001, w_onCk_effect_sum=-114.3463400974954, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -114.3463400974954 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -4.764430837395644\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: 0.0021707662737773794 (lr=0.001, w_onCk_effect_sum=2.1707662737773794, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: 2.1707662737773794 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: 0.09044859474072421\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: 0.0005178002051526897 (lr=0.001, w_onCk_effect_sum=0.5178002051526897, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: 0.5178002051526897 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: 0.004045314102755429\n",
      "\n",
      "Batch 2 / 234 completed of epoch 0 / 1. Loss: 8.519462691805208, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208]\n",
      "Batch label balance: Counter({2: 28, 7: 28, 4: 28, 8: 27, 5: 26, 1: 26, 0: 25, 9: 24, 6: 24, 3: 20})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.1759922414017252 (lr=0.001, w_onCk_effect_sum=-1175.992241401725, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1175.992241401725 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -3240662.859026196\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -0.6913953080828164 (lr=0.001, w_onCk_effect_sum=-691.3953080828163, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -691.3953080828163 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -28.80813783678401\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -0.03652771938503237 (lr=0.001, w_onCk_effect_sum=-36.52771938503237, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -36.52771938503237 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1.521988307709685\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -0.010681513287262424 (lr=0.001, w_onCk_effect_sum=-10.681513287262424, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -10.681513287262424 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -0.08344932255673826\n",
      "\n",
      "Batch 3 / 234 completed of epoch 0 / 1. Loss: 8.189727020189405, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405]\n",
      "Batch label balance: Counter({7: 40, 4: 28, 5: 25, 6: 25, 0: 25, 1: 25, 8: 23, 3: 23, 9: 21, 2: 21})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.157991426248797 (lr=0.001, w_onCk_effect_sum=-1157.9914262487969, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1157.9914262487969 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -6679006.158827652\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -3.696006401849999 (lr=0.001, w_onCk_effect_sum=-3696.0064018499993, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -3696.0064018499993 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -154.00026674374993\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -2.6385331818504203 (lr=0.001, w_onCk_effect_sum=-2638.53318185042, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -2638.53318185042 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -109.93888257710111\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -1.0248376114663407 (lr=0.001, w_onCk_effect_sum=-1024.8376114663406, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1024.8376114663406 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -8.006543839580724\n",
      "\n",
      "Batch 4 / 234 completed of epoch 0 / 1. Loss: 6.835321738344102, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102]\n",
      "Batch label balance: Counter({4: 34, 1: 32, 7: 28, 9: 26, 8: 25, 2: 25, 3: 24, 6: 23, 5: 20, 0: 19})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.192992639472152 (lr=0.001, w_onCk_effect_sum=-1192.992639472152, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1192.992639472152 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -212064153.61909202\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -5.189196387380889 (lr=0.001, w_onCk_effect_sum=-5189.196387380889, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -5189.196387380889 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -216.21651614087028\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -23.214875512422093 (lr=0.001, w_onCk_effect_sum=-23214.87551242209, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -23214.87551242209 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -967.2864796842528\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -74.82969950837634 (lr=0.001, w_onCk_effect_sum=-74829.69950837633, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -74829.69950837633 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -584.6070274091907\n",
      "\n",
      "Batch 5 / 234 completed of epoch 0 / 1. Loss: 5.9485007983238845, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845]\n",
      "Batch label balance: Counter({7: 30, 9: 30, 5: 30, 6: 28, 0: 27, 3: 25, 1: 24, 8: 22, 4: 20, 2: 20})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.1419993000523805 (lr=0.001, w_onCk_effect_sum=-1141.9993000523805, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1141.9993000523805 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -54776641575.83467\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -6.329785866394185 (lr=0.001, w_onCk_effect_sum=-6329.785866394185, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -6329.785866394185 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -263.7410777664244\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -61.16402550144188 (lr=0.001, w_onCk_effect_sum=-61164.02550144187, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -61164.02550144187 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -2548.501062560081\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -1617.0683798796836 (lr=0.001, w_onCk_effect_sum=-1617068.3798796835, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1617068.3798796835 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -12633.34671781021\n",
      "\n",
      "Batch 6 / 234 completed of epoch 0 / 1. Loss: 7.2215982385720165, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165]\n",
      "Batch label balance: Counter({4: 31, 2: 31, 6: 29, 9: 29, 1: 26, 3: 26, 7: 23, 0: 23, 8: 22, 5: 16})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.1579999965144134 (lr=0.001, w_onCk_effect_sum=-1157.9999965144134, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1157.9999965144134 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -6467357670862.408\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -7.7409084812914974 (lr=0.001, w_onCk_effect_sum=-7740.908481291497, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -7740.908481291497 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -322.5378533871457\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -123.79784141562843 (lr=0.001, w_onCk_effect_sum=-123797.84141562843, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -123797.84141562843 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -5158.243392317861\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -10844.969741449131 (lr=0.001, w_onCk_effect_sum=-10844969.74144913, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -10844969.74144913 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -84726.32610507261\n",
      "\n",
      "Batch 7 / 234 completed of epoch 0 / 1. Loss: 9.748417212255356, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356]\n",
      "Batch label balance: Counter({6: 30, 9: 27, 4: 27, 5: 26, 3: 26, 2: 26, 7: 26, 1: 25, 0: 23, 8: 20})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.124999999965832 (lr=0.001, w_onCk_effect_sum=-1124.9999999658319, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1124.9999999658319 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -155925283682456.38\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -8.82306266218865 (lr=0.001, w_onCk_effect_sum=-8823.062662188651, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -8823.062662188651 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -367.62761092452723\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -209.40290322345334 (lr=0.001, w_onCk_effect_sum=-209402.90322345332, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -209402.90322345332 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -8725.12096764391\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -44267.7933717876 (lr=0.001, w_onCk_effect_sum=-44267793.3717876, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -44267793.3717876 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -345842.1357170935\n",
      "\n",
      "Batch 8 / 234 completed of epoch 0 / 1. Loss: 12.870461523502364, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364]\n",
      "Batch label balance: Counter({4: 34, 6: 30, 3: 30, 7: 26, 5: 25, 8: 25, 0: 24, 1: 24, 2: 22, 9: 16})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.093999999998449 (lr=0.001, w_onCk_effect_sum=-1093.999999998449, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1093.999999998449 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1751888210180448.8\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -9.810688269039812 (lr=0.001, w_onCk_effect_sum=-9810.68826903981, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -9810.68826903981 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -408.7786778766589\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -319.40311637289824 (lr=0.001, w_onCk_effect_sum=-319403.1163728982, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -319403.1163728982 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -13308.463182204092\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -134405.78861058754 (lr=0.001, w_onCk_effect_sum=-134405788.61058754, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -134405788.61058754 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1050045.2235201981\n",
      "\n",
      "Batch 9 / 234 completed of epoch 0 / 1. Loss: 13.911191786965666, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666]\n",
      "Batch label balance: Counter({1: 30, 4: 30, 2: 29, 0: 26, 6: 26, 7: 26, 5: 25, 3: 22, 9: 21, 8: 21})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.1279999999998518 (lr=0.001, w_onCk_effect_sum=-1127.9999999998518, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1127.9999999998518 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1.486752836977056e+16\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -11.349622829514718 (lr=0.001, w_onCk_effect_sum=-11349.622829514718, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -11349.622829514718 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -472.90095122978016\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -480.8532774123154 (lr=0.001, w_onCk_effect_sum=-480853.27741231536, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -480853.27741231536 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -20035.553225513173\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -355930.5355605323 (lr=0.001, w_onCk_effect_sum=-355930535.5605323, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -355930535.5605323 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -2780707.3090667143\n",
      "\n",
      "Batch 10 / 234 completed of epoch 0 / 1. Loss: 14.669169386394273, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273]\n",
      "Batch label balance: Counter({1: 33, 3: 29, 7: 28, 9: 27, 2: 26, 6: 24, 4: 23, 0: 23, 8: 22, 5: 21})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.0809999999999784 (lr=0.001, w_onCk_effect_sum=-1080.9999999999784, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1080.9999999999784 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -7.94558880574543e+16\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -12.096089878286007 (lr=0.001, w_onCk_effect_sum=-12096.089878286006, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -12096.089878286006 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -504.0037449285835\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -649.7651550848674 (lr=0.001, w_onCk_effect_sum=-649765.1550848674, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -649765.1550848674 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -27073.54812853616\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -793401.8444005012 (lr=0.001, w_onCk_effect_sum=-793401844.4005013, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -793401844.4005013 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -6198451.90937898\n",
      "\n",
      "Batch 11 / 234 completed of epoch 0 / 1. Loss: 16.147886342791868, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868]\n",
      "Batch label balance: Counter({1: 31, 0: 29, 4: 27, 6: 27, 3: 26, 2: 25, 7: 25, 8: 25, 5: 23, 9: 18})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.2299999999999995 (lr=0.001, w_onCk_effect_sum=-1229.9999999999995, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1229.9999999999995 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -3.83069739600959e+17\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -15.092988510908357 (lr=0.001, w_onCk_effect_sum=-15092.988510908357, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -15092.988510908357 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -628.8745212878486\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -993.3155793987607 (lr=0.001, w_onCk_effect_sum=-993315.5793987607, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -993315.5793987607 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -41388.149141615126\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -1858319.1677024334 (lr=0.001, w_onCk_effect_sum=-1858319167.7024333, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1858319167.7024333 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -14518118.497675085\n",
      "\n",
      "Batch 12 / 234 completed of epoch 0 / 1. Loss: 15.61715347202204, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204]\n",
      "Batch label balance: Counter({9: 33, 4: 30, 7: 30, 2: 28, 8: 27, 6: 23, 3: 22, 1: 22, 5: 21, 0: 20})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.1159999999999999 (lr=0.001, w_onCk_effect_sum=-1115.9999999999998, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1115.9999999999998 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1.585741008252428e+18\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -15.066806161116915 (lr=0.001, w_onCk_effect_sum=-15066.806161116914, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -15066.806161116914 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -627.7835900465384\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -1218.9955714374157 (lr=0.001, w_onCk_effect_sum=-1218995.5714374157, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1218995.5714374157 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -50791.48214322573\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -3491374.128639725 (lr=0.001, w_onCk_effect_sum=-3491374128.6397247, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -3491374128.6397247 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -27276360.379997544\n",
      "\n",
      "Batch 13 / 234 completed of epoch 0 / 1. Loss: 15.95122569657339, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339]\n",
      "Batch label balance: Counter({1: 34, 3: 28, 8: 28, 4: 27, 0: 26, 7: 24, 9: 24, 6: 23, 2: 22, 5: 20})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.163 (lr=0.001, w_onCk_effect_sum=-1163.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1163.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -6.349619722776354e+18\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -16.999248112346734 (lr=0.001, w_onCk_effect_sum=-16999.248112346733, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -16999.248112346733 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -708.3020046811141\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -1631.4661669569257 (lr=0.001, w_onCk_effect_sum=-1631466.1669569258, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1631466.1669569258 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -67977.75695653871\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -6661497.744217928 (lr=0.001, w_onCk_effect_sum=-6661497744.217927, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -6661497744.217927 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -52042951.126701996\n",
      "\n",
      "Batch 14 / 234 completed of epoch 0 / 1. Loss: 16.53060856075257, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257]\n",
      "Batch label balance: Counter({9: 28, 3: 28, 8: 27, 6: 27, 0: 26, 4: 26, 1: 25, 2: 25, 7: 25, 5: 19})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.22 (lr=0.001, w_onCk_effect_sum=-1220.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1220.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -2.139454728290914e+19\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -19.251261287242503 (lr=0.001, w_onCk_effect_sum=-19251.2612872425, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -19251.2612872425 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -802.135886968438\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -2174.8552404923785 (lr=0.001, w_onCk_effect_sum=-2174855.2404923784, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -2174855.2404923784 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -90618.968353849\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -12428431.810445465 (lr=0.001, w_onCk_effect_sum=-12428431810.445465, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -12428431810.445465 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -97097123.51910433\n",
      "\n",
      "Batch 15 / 234 completed of epoch 0 / 1. Loss: 16.09722317017529, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529]\n",
      "Batch label balance: Counter({9: 31, 7: 30, 6: 30, 1: 30, 8: 29, 2: 25, 0: 22, 5: 22, 3: 19, 4: 18})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.112 (lr=0.001, w_onCk_effect_sum=-1112.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1112.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -5.66938194869874e+19\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -18.903691271650565 (lr=0.001, w_onCk_effect_sum=-18903.691271650565, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -18903.691271650565 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -787.6538029854399\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -2499.5094290533043 (lr=0.001, w_onCk_effect_sum=-2499509.4290533043, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -2499509.4290533043 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -104146.22621055442\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -19719772.421868093 (lr=0.001, w_onCk_effect_sum=-19719772421.86809, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -19719772421.86809 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -154060722.04584166\n",
      "\n",
      "Batch 16 / 234 completed of epoch 0 / 1. Loss: 15.819243895394614, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614]\n",
      "Batch label balance: Counter({7: 31, 4: 30, 3: 30, 1: 29, 0: 29, 9: 25, 5: 21, 6: 21, 2: 20, 8: 20})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.129 (lr=0.001, w_onCk_effect_sum=-1129.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1129.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1.609639778608206e+20\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -20.4481345518826 (lr=0.001, w_onCk_effect_sum=-20448.1345518826, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -20448.1345518826 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -852.0056063284422\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -3090.2661195115306 (lr=0.001, w_onCk_effect_sum=-3090266.1195115307, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -3090266.1195115307 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -128761.08831298082\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -32104671.29421529 (lr=0.001, w_onCk_effect_sum=-32104671294.21529, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -32104671294.21529 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -250817744.48605412\n",
      "\n",
      "Batch 17 / 234 completed of epoch 0 / 1. Loss: 16.178529818992864, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864]\n",
      "Batch label balance: Counter({7: 33, 1: 29, 4: 28, 2: 27, 3: 26, 6: 26, 0: 23, 8: 23, 5: 21, 9: 20})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.137 (lr=0.001, w_onCk_effect_sum=-1137.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1137.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -4.7105211483712415e+20\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -21.876701330815347 (lr=0.001, w_onCk_effect_sum=-21876.701330815347, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -21876.701330815347 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -911.5292221173058\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -3753.4989255895416 (lr=0.001, w_onCk_effect_sum=-3753498.9255895414, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -3753498.9255895414 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -156395.78856623117\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -50594285.27963693 (lr=0.001, w_onCk_effect_sum=-50594285279.636925, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -50594285279.636925 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -395267853.7471655\n",
      "\n",
      "Batch 18 / 234 completed of epoch 0 / 1. Loss: 16.61060820848948, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948]\n",
      "Batch label balance: Counter({4: 33, 8: 28, 2: 27, 0: 26, 1: 26, 6: 25, 7: 24, 9: 23, 5: 22, 3: 22})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.117 (lr=0.001, w_onCk_effect_sum=-1117.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1117.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1.0094846971940046e+21\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -22.7619158834835 (lr=0.001, w_onCk_effect_sum=-22761.9158834835, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -22761.9158834835 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -948.4131618118123\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -4403.335405560296 (lr=0.001, w_onCk_effect_sum=-4403335.405560296, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -4403335.405560296 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -183472.3085650125\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -75881497.09314756 (lr=0.001, w_onCk_effect_sum=-75881497093.14755, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -75881497093.14755 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -592824196.040221\n",
      "\n",
      "Batch 19 / 234 completed of epoch 0 / 1. Loss: 15.718581042148438, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438]\n",
      "Batch label balance: Counter({1: 36, 7: 30, 3: 27, 0: 27, 6: 26, 4: 25, 9: 25, 8: 22, 2: 19, 5: 19})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.099 (lr=0.001, w_onCk_effect_sum=-1099.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1099.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -2.197717598056485e+21\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -23.622699880884785 (lr=0.001, w_onCk_effect_sum=-23622.699880884786, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -23622.699880884786 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -984.2791617035326\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -5107.55359521256 (lr=0.001, w_onCk_effect_sum=-5107553.59521256, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -5107553.59521256 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -212814.73313385705\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -110507371.70956227 (lr=0.001, w_onCk_effect_sum=-110507371709.56227, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -110507371709.56227 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -863338841.4809569\n",
      "\n",
      "Batch 20 / 234 completed of epoch 0 / 1. Loss: 16.81088116553371, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371]\n",
      "Batch label balance: Counter({0: 36, 1: 27, 3: 27, 7: 27, 6: 26, 9: 26, 2: 25, 8: 25, 4: 23, 5: 14})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.172 (lr=0.001, w_onCk_effect_sum=-1172.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1172.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -5.236314283893511e+21\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -26.47984261364607 (lr=0.001, w_onCk_effect_sum=-26479.84261364607, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -26479.84261364607 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1103.326775568586\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -6350.832643978745 (lr=0.001, w_onCk_effect_sum=-6350832.643978745, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -6350832.643978745 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -264618.02683244686\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -169844258.59161672 (lr=0.001, w_onCk_effect_sum=-169844258591.61673, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -169844258591.61673 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1326908270.2469802\n",
      "\n",
      "Batch 21 / 234 completed of epoch 0 / 1. Loss: 16.92454629986834, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371, 16.92454629986834]\n",
      "Batch label balance: Counter({3: 32, 9: 30, 7: 29, 0: 28, 5: 26, 1: 25, 8: 24, 6: 24, 2: 20, 4: 18})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.111 (lr=0.001, w_onCk_effect_sum=-1111.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1111.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -9.574299028956641e+21\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -26.403717549283915 (lr=0.001, w_onCk_effect_sum=-26403.717549283912, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -26403.717549283912 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1100.1548978868295\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -7031.741360471006 (lr=0.001, w_onCk_effect_sum=-7031741.3604710065, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -7031741.3604710065 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -292989.22335295874\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -232711635.53054848 (lr=0.001, w_onCk_effect_sum=-232711635530.5485, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -232711635530.5485 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1818059652.5824268\n",
      "\n",
      "Batch 22 / 234 completed of epoch 0 / 1. Loss: 16.70107787781185, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371, 16.92454629986834, 16.70107787781185]\n",
      "Batch label balance: Counter({1: 32, 3: 31, 6: 30, 9: 28, 2: 25, 0: 25, 7: 23, 5: 23, 4: 21, 8: 18})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.115 (lr=0.001, w_onCk_effect_sum=-1115.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1115.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -2.1724655840773116e+22\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -27.737545438750352 (lr=0.001, w_onCk_effect_sum=-27737.545438750352, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -27737.545438750352 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1155.7310599479308\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -8119.335834445586 (lr=0.001, w_onCk_effect_sum=-8119335.834445586, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -8119335.834445586 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -338305.65976856643\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -325798049.58175087 (lr=0.001, w_onCk_effect_sum=-325798049581.75085, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -325798049581.75085 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -2545297262.3574224\n",
      "\n",
      "Batch 23 / 234 completed of epoch 0 / 1. Loss: 16.599163478813004, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371, 16.92454629986834, 16.70107787781185, 16.599163478813004]\n",
      "Batch label balance: Counter({2: 32, 1: 29, 3: 29, 7: 28, 6: 27, 9: 27, 0: 24, 4: 23, 8: 19, 5: 18})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.168 (lr=0.001, w_onCk_effect_sum=-1168.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1168.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -4.44230750780835e+22\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -30.358331724179685 (lr=0.001, w_onCk_effect_sum=-30358.331724179683, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -30358.331724179683 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1264.9304885074866\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -9728.558146464387 (lr=0.001, w_onCk_effect_sum=-9728558.146464387, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -9728558.146464387 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -405356.58943601535\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -469359447.9780029 (lr=0.001, w_onCk_effect_sum=-469359447978.00287, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -469359447978.00287 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -3666870687.3281274\n",
      "\n",
      "Batch 24 / 234 completed of epoch 0 / 1. Loss: 16.71022976450579, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371, 16.92454629986834, 16.70107787781185, 16.599163478813004, 16.71022976450579]\n",
      "Batch label balance: Counter({1: 29, 9: 28, 5: 28, 3: 26, 7: 25, 6: 25, 8: 25, 2: 24, 4: 24, 0: 22})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.112 (lr=0.001, w_onCk_effect_sum=-1112.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1112.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -6.9138216863361734e+22\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -30.201611271650506 (lr=0.001, w_onCk_effect_sum=-30201.611271650505, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -30201.611271650505 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1258.4004696521051\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -10595.20641985123 (lr=0.001, w_onCk_effect_sum=-10595206.41985123, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -10595206.41985123 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -441466.93416046794\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -614247435.3426636 (lr=0.001, w_onCk_effect_sum=-614247435342.6636, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -614247435342.6636 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -4798808088.614613\n",
      "\n",
      "Batch 25 / 234 completed of epoch 0 / 1. Loss: 17.4393173468891, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371, 16.92454629986834, 16.70107787781185, 16.599163478813004, 16.71022976450579, 17.4393173468891]\n",
      "Batch label balance: Counter({3: 32, 1: 29, 6: 28, 8: 27, 0: 25, 2: 25, 5: 24, 7: 23, 4: 23, 9: 20})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.171 (lr=0.001, w_onCk_effect_sum=-1171.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1171.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1.1699385532445164e+23\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -33.106186891279464 (lr=0.001, w_onCk_effect_sum=-33106.18689127946, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -33106.18689127946 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1379.4244538033104\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -12614.038011409011 (lr=0.001, w_onCk_effect_sum=-12614038.01140901, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -12614038.01140901 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -525584.9171420429\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -864935693.3469921 (lr=0.001, w_onCk_effect_sum=-864935693346.9921, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -864935693346.9921 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -6757310104.273417\n",
      "\n",
      "Batch 26 / 234 completed of epoch 0 / 1. Loss: 16.747673893013147, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371, 16.92454629986834, 16.70107787781185, 16.599163478813004, 16.71022976450579, 17.4393173468891, 16.747673893013147]\n",
      "Batch label balance: Counter({7: 30, 6: 29, 3: 28, 4: 27, 1: 27, 8: 25, 5: 25, 0: 24, 9: 23, 2: 18})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.116 (lr=0.001, w_onCk_effect_sum=-1116.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1116.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -2.0976858619085588e+23\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -32.85807816111685 (lr=0.001, w_onCk_effect_sum=-32858.07816111685, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -32858.07816111685 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1369.0865900465349\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -13607.309908956035 (lr=0.001, w_onCk_effect_sum=-13607309.908956034, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -13607309.908956034 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -566971.246206502\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -1104686771.5140634 (lr=0.001, w_onCk_effect_sum=-1104686771514.0632, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1104686771514.0632 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -8630365402.453785\n",
      "\n",
      "Batch 27 / 234 completed of epoch 0 / 1. Loss: 17.14472164258815, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371, 16.92454629986834, 16.70107787781185, 16.599163478813004, 16.71022976450579, 17.4393173468891, 16.747673893013147, 17.14472164258815]\n",
      "Batch label balance: Counter({0: 34, 1: 30, 8: 29, 9: 28, 5: 26, 3: 23, 2: 23, 7: 21, 6: 21, 4: 21})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.1460000000000001 (lr=0.001, w_onCk_effect_sum=-1146.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1146.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -3.372892831686231e+23\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -35.020295832114684 (lr=0.001, w_onCk_effect_sum=-35020.29583211468, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -35020.29583211468 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1459.178993004778\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -15653.435174950613 (lr=0.001, w_onCk_effect_sum=-15653435.174950613, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -15653435.174950613 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -652226.4656229417\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -1483799183.899468 (lr=0.001, w_onCk_effect_sum=-1483799183899.468, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1483799183899.468 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -11592181124.214664\n",
      "\n",
      "Batch 28 / 234 completed of epoch 0 / 1. Loss: 16.739470760398053, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371, 16.92454629986834, 16.70107787781185, 16.599163478813004, 16.71022976450579, 17.4393173468891, 16.747673893013147, 17.14472164258815, 16.739470760398053]\n",
      "Batch label balance: Counter({7: 32, 2: 29, 1: 28, 9: 26, 5: 26, 0: 25, 3: 24, 8: 23, 6: 22, 4: 21})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.1 (lr=0.001, w_onCk_effect_sum=-1100.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1100.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -6.180972971557327e+23\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -34.87519460325136 (lr=0.001, w_onCk_effect_sum=-34875.19460325136, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -34875.19460325136 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1453.1331084688074\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -16809.917197426003 (lr=0.001, w_onCk_effect_sum=-16809917.197426002, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -16809917.197426002 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -700413.2165594179\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -1856555807.463745 (lr=0.001, w_onCk_effect_sum=-1856555807463.745, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1856555807463.745 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -14504342245.810545\n",
      "\n",
      "Batch 29 / 234 completed of epoch 0 / 1. Loss: 17.102144287851925, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371, 16.92454629986834, 16.70107787781185, 16.599163478813004, 16.71022976450579, 17.4393173468891, 16.747673893013147, 17.14472164258815, 16.739470760398053, 17.102144287851925]\n",
      "Batch label balance: Counter({1: 35, 2: 32, 8: 30, 3: 26, 6: 25, 7: 24, 0: 23, 5: 21, 4: 20, 9: 20})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.108 (lr=0.001, w_onCk_effect_sum=-1108.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1108.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1.104610892871342e+24\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -36.34763238218421 (lr=0.001, w_onCk_effect_sum=-36347.63238218421, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -36347.63238218421 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1514.4846825910097\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -18787.26605847392 (lr=0.001, w_onCk_effect_sum=-18787266.05847392, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -18787266.05847392 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -782802.7524364125\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -2390754666.272849 (lr=0.001, w_onCk_effect_sum=-2390754666272.849, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -2390754666272.849 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -18677770830.25641\n",
      "\n",
      "Batch 30 / 234 completed of epoch 0 / 1. Loss: 16.965707159572073, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371, 16.92454629986834, 16.70107787781185, 16.599163478813004, 16.71022976450579, 17.4393173468891, 16.747673893013147, 17.14472164258815, 16.739470760398053, 17.102144287851925, 16.965707159572073]\n",
      "Batch label balance: Counter({1: 32, 2: 31, 8: 29, 0: 29, 9: 27, 7: 23, 3: 23, 5: 21, 4: 21, 6: 20})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.102 (lr=0.001, w_onCk_effect_sum=-1102.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1102.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1.7772941471311946e+24\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -37.37182004798466 (lr=0.001, w_onCk_effect_sum=-37371.820047984664, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -37371.820047984664 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1557.1591686660283\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -20675.02258961696 (lr=0.001, w_onCk_effect_sum=-20675022.58961696, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -20675022.58961696 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -861459.2745673732\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -3019406377.127147 (lr=0.001, w_onCk_effect_sum=-3019406377127.147, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -3019406377127.147 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -23589112321.3061\n",
      "\n",
      "Batch 31 / 234 completed of epoch 0 / 1. Loss: 16.94683699178974, Loss history: [17.55464100678961, 9.958949221937699, 8.519462691805208, 8.189727020189405, 6.835321738344102, 5.9485007983238845, 7.2215982385720165, 9.748417212255356, 12.870461523502364, 13.911191786965666, 14.669169386394273, 16.147886342791868, 15.61715347202204, 15.95122569657339, 16.53060856075257, 16.09722317017529, 15.819243895394614, 16.178529818992864, 16.61060820848948, 15.718581042148438, 16.81088116553371, 16.92454629986834, 16.70107787781185, 16.599163478813004, 16.71022976450579, 17.4393173468891, 16.747673893013147, 17.14472164258815, 16.739470760398053, 17.102144287851925, 16.965707159572073, 16.94683699178974]\n",
      "Batch label balance: Counter({1: 32, 0: 32, 6: 31, 7: 30, 3: 29, 9: 23, 5: 22, 8: 20, 2: 19, 4: 18})\n",
      "Backpropagating layer 3\n",
      "Weight change: -1.171 (lr=0.001, w_onCk_effect_sum=-1171.0, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -1171.0 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -2.9782612570707786e+24\n",
      "\n",
      "Backpropagating layer 2\n",
      "Weight change: -41.002239891279466 (lr=0.001, w_onCk_effect_sum=-41002.23989127947, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -41002.23989127947 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1708.426662136645\n",
      "\n",
      "Backpropagating layer 1\n",
      "Weight change: -24215.789689865054 (lr=0.001, w_onCk_effect_sum=-24215789.689865053, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -24215789.689865053 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n",
      "Average (of layer) of the sum of effect of weight change cost: -1008991.2370777125\n",
      "\n",
      "Backpropagating layer 0\n",
      "Weight change: -4037166469.4059124 (lr=0.001, w_onCk_effect_sum=-4037166469405.912, batch_size=256) for weight W[0][0]\n",
      "Sum of effect of weight change on costs: -4037166469405.912 (for Neuron index j=0 Previous activation index k=0 Example index=255)\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(\n",
    "    LinearLayer(\n",
    "        neurons=128,\n",
    "        inputs=784,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_categorical_cross_entropy_loss\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=24,\n",
    "        inputs=128,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_categorical_cross_entropy_loss\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=24,\n",
    "        inputs=24,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_categorical_cross_entropy_loss\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=10,\n",
    "        inputs=24,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_categorical_cross_entropy_loss\n",
    "    )\n",
    ")\n",
    "\n",
    "nn.train(\n",
    "    X=train_set.get_linearized_images(),\n",
    "    y=train_set.labels,\n",
    "    epochs=1,\n",
    "    batch_size=256,\n",
    "    learning_rate=0.001\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-08T11:21:07.565922600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-08T16:48:47.309964700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = nn.predict_multiple(test_set.get_linearized_images())\n",
    "print(\"Classification report:\\n\", classification_report(test_set.labels, predictions))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(test_set.labels, predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-08T16:48:47.311932400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "layer = LinearLayer(neurons=4, inputs=3, activation_function=relu, derivative_activation_function=relu_derivative,\n",
    "                    derivative_cost_function=derivative_categorical_cross_entropy_loss, test_mode=True)\n",
    "result = layer.forward([1, 2, 3])\n",
    "assert result == [3.5, 3.5, 3.5, 3.5]\n",
    "pretty_print_matrix(result)\n",
    "\n",
    "loss = mean_squared_error_loss_categorical(result, 2)\n",
    "assert loss == 10.7101\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-08T16:48:47.312932400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(\n",
    "    LinearLayer(\n",
    "        neurons=256,\n",
    "        inputs=784,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_categorical_cross_entropy_loss,\n",
    "        test_mode=True\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=128,\n",
    "        inputs=256,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_categorical_cross_entropy_loss,\n",
    "        test_mode=True\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=64,\n",
    "        inputs=128,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_categorical_cross_entropy_loss,\n",
    "        test_mode=True\n",
    "    ),\n",
    "    LinearLayer(\n",
    "        neurons=10,\n",
    "        inputs=64,\n",
    "        activation_function=relu,\n",
    "        derivative_activation_function=relu_derivative,\n",
    "        derivative_cost_function=derivative_categorical_cross_entropy_loss,\n",
    "        test_mode=True\n",
    "    )\n",
    ")\n",
    "\n",
    "prediction = softmax(\n",
    "    nn.predict(test_set.images[0].get_linearized())\n",
    ")\n",
    "test_set.images[0].print()\n",
    "pretty_print_matrix(softmax(prediction))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-08T16:48:47.313934100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-08T16:48:47.314933300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
